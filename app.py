# -*- coding: utf-8 -*-
# Herramienta para la elaboraci√≥n de bibliograf√≠as especializadas
# v8.2.4-b ‚Äì Ajustes sincronizaci√≥n + ocultar columna inicial

import io
import os
import time
import tempfile
from typing import List, Dict, Any

import pandas as pd
import requests
import streamlit as st
import re

# ---------------------------------- CONFIGURACI√ìN B√ÅSICA ----------------------------------
st.set_page_config(page_title="Herramienta de bibliograf√≠as", layout="wide")

LOGO_URL = "https://biblioteca.unbosque.edu.co/sites/default/files/Logos/Logo%201%20Blanco.png"

# URLs oficiales (Digital/F√≠sica y plantillas)
URL_DIGITAL = "https://biblioteca.unbosque.edu.co/sites/default/files/Formatos-Biblioteca/Biblioteca%20Colecci%C3%B3n%20Digital.xlsx"
URL_FISICA = "https://biblioteca.unbosque.edu.co/sites/default/files/Formatos-Biblioteca/Biblioteca%20BD%20Colecci%C3%B3n%20F%C3%ADsica.xlsx"

URL_PLANTILLA_TEMATICAS = "https://biblioteca.unbosque.edu.co/sites/default/files/Formatos-Biblioteca/Plantilla%20Tem%C3%A1ticas.xlsx"
URL_PLANTILLA_EXCLUSION = "https://biblioteca.unbosque.edu.co/sites/default/files/Formatos-Biblioteca/Plantilla%20T%C3%A9rminos%20a%20excluir.xlsx"

DEFAULT_COL_TITULO = "T√≠tulo"
DEFAULT_COL_TEMATICAS = "Tem√°ticas"
DEFAULT_DUP_DIGITAL = "Url OA"
DEFAULT_DUP_FISICA = "No. Topogr√°fico"

UA = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari"  # noqa: E501
}

# Columnas a OMITIR en exportaciones CSV/XLSX
EXPORT_DROP_COLS = {
    "Fecha de actualizaci√≥n",
    "Tipo de √≠tem normalizado mat especial",
    "Formato",
    "Prioridad B√∫squeda",
}

# Renombres para exportaci√≥n
EXPORT_RENAME = {
    "Tem√°ticas": "Tem√°ticas catalogadas por el Editor",
    "Tem√°tica": "T√©rmino de b√∫squeda",
    "Tem√°tica normalizada": "T√©rmino de b√∫squeda normalizado",
    "Url en LOCATE/IDEA": "Url de acceso",
}

TIPO_NORMAL_COL = "Tipo de √≠tem normalizado mat especial"

# ---------------------------------- ESTADO GLOBAL ----------------------------------
ss = st.session_state

ss.setdefault("df_digital", None)
ss.setdefault("df_fisica", None)
ss.setdefault("bases_ready", False)

ss.setdefault("downloading", False)          # s√≥lo para deshabilitar controles
ss.setdefault("descarga_disparada", False)

# Insumos m√©todo A
ss.setdefault("tematicas_df", None)
ss.setdefault("excluir_df", None)

# Resultados comunes (A y B)
ss.setdefault("results_df", None)
ss.setdefault("bitacora_df", None)

# M√©todo de b√∫squeda actual: "A" (tem√°ticas) o "B" (avanzada)
ss.setdefault("metodo", "A")

# Estado m√©todo B
ss.setdefault("b_num_cond", 2)
ss.setdefault("b_conds", [])


# ---------------------------------- UTILIDADES ----------------------------------
def normalize_text(s: Any) -> str:
    if pd.isna(s):
        return ""
    s = str(s)
    return (
        s.replace("\u0301", "")
        .replace("\u0303", "")
        .replace("\u2019", "'")
        .replace("\xa0", " ")
        .strip()
    )


def _head_content_length(url: str, timeout: int = 30) -> int | None:
    try:
        r = requests.head(url, allow_redirects=True, timeout=timeout, headers=UA)
        r.raise_for_status()
        cl = r.headers.get("Content-Length")
        return int(cl) if cl is not None else None
    except Exception:
        return None


def download_with_resume(
    url: str,
    label: str,
    container=None,
    max_retries: int = 5,
    chunk_size: int = 256 * 1024,
    timeout: int = 300,
) -> io.BytesIO:
    where = container if container is not None else st
    status = where.empty()
    bar = where.progress(0)
    info = where.empty()

    tmp_dir = tempfile.gettempdir()
    tmp_path = os.path.join(tmp_dir, f"dl_{abs(hash(url))}.part")

    total_size = _head_content_length(url)
    attempt = 0

    while attempt < max_retries:
        attempt += 1
        try:
            downloaded = os.path.getsize(tmp_path) if os.path.exists(tmp_path) else 0
            headers = dict(UA)
            mode = "wb"

            if downloaded and total_size and downloaded < total_size:
                headers["Range"] = f"bytes={downloaded}-"
                mode = "ab"

            status.info(f"Descargando {label}‚Ä¶ (intento {attempt}/{max_retries})")

            with requests.get(
                url,
                stream=True,
                headers=headers,
                timeout=timeout,
                allow_redirects=True,
            ) as r:
                if headers.get("Range") and r.status_code == 200:
                    if os.path.exists(tmp_path):
                        os.remove(tmp_path)
                    downloaded = 0
                    mode = "wb"

                r.raise_for_status()

                content_length = r.headers.get("Content-Length")
                expected_total = downloaded + int(content_length) if content_length else total_size  # noqa: E501

                last = time.time()
                with open(tmp_path, mode) as f:
                    for chunk in r.iter_content(chunk_size=chunk_size):
                        if not chunk:
                            continue
                        f.write(chunk)
                        downloaded += len(chunk)
                        if expected_total and time.time() - last > 0.1:
                            bar.progress(min(1.0, downloaded / expected_total))
                            info.write(
                                f"{downloaded/1e6:,.1f} MB / {expected_total/1e6:,.1f} MB"
                            )
                            last = time.time()

            if total_size and downloaded < total_size:
                raise requests.exceptions.ChunkedEncodingError(
                    f"Descarga incompleta: {downloaded} de {total_size} bytes"
                )

            bar.progress(1.0)
            status.success(f"{label} descargado correctamente.")
            info.empty()
            bar.empty()
            status.empty()

            with open(tmp_path, "rb") as f:
                data = f.read()
            return io.BytesIO(data)

        except Exception as e:
            info.empty()
            bar.empty()
            status.warning(f"Fallo al descargar {label}: {e}")
            if attempt < max_retries:
                time.sleep(2)
            else:
                status.error(
                    f"No se pudo descargar {label} tras {max_retries} intentos."
                )
                raise
        finally:
            info.empty()
            bar.empty()
            status.empty()


def safe_read_excel(bio_or_file, label: str = "archivo") -> pd.DataFrame:
    try:
        with st.spinner(f"Procesando {label}‚Ä¶"):
            df = pd.read_excel(bio_or_file, engine="openpyxl", dtype=str)
            if not isinstance(df, pd.DataFrame):
                raise ValueError("El archivo no es una hoja de c√°lculo v√°lida.")
            df = df.fillna("")
            return df
    except Exception as e:
        raise RuntimeError(f"No fue posible procesar {label}: {e}") from e


def get_index_or_first(options: List[str], value: str) -> int:
    try:
        return options.index(value)
    except Exception:
        return 0


def _prepara_columnas(df: pd.DataFrame, cols: List[str]):
    for c in cols:
        if c in df.columns:
            df[c] = df[c].astype(str).fillna("")


def _prep_export(df: pd.DataFrame) -> pd.DataFrame:
    """
    Renombra columnas y elimina las administrativas antes de exportar.
    """
    out = df.copy()
    out = out.rename(
        columns={k: v for k, v in EXPORT_RENAME.items() if k in out.columns}
    )
    if "Url en LOCATE/IDEA" in out.columns and "Url de acceso" not in out.columns:
        out = out.rename(columns={"Url en LOCATE/IDEA": "Url de acceso"})

    drop_cols = [c for c in out.columns if c in EXPORT_DROP_COLS]
    drop_cols += [c for c in out.columns if c.startswith("Unnamed")]
    drop_cols += [c for c in out.columns if not str(c).strip()]  # columnas sin nombre
    if drop_cols:
        out = out.drop(columns=list(dict.fromkeys(drop_cols)))
    return out.fillna("")


def _clean_field(value: Any) -> str:
    """
    Limpia campos para citas APA: elimina vac√≠os, 'nan', 'NO APLICA', etc.
    """
    if value is None:
        return ""
    v = str(value).strip()
    if not v:
        return ""
    if v.lower() in ("nan", "none", "null"):
        return ""
    if v.upper() in ("NO APLICA", "N/A"):
        return ""
    return v


def build_apa(row: pd.Series) -> str:
    """
    Generador APA simplificado usando los campos disponibles.
    """
    tit = _clean_field(row.get("T√≠tulo", ""))

    # Autor(es) puede venir con o sin espacio final
    aut = ""
    for col in ["Autor(es)", "Autor(es) "]:
        if col in row.index:
            cand = _clean_field(row.get(col, ""))
            if cand:
                aut = cand
                break

    edit = _clean_field(row.get("Editorial", ""))
    anio = _clean_field(row.get("A√±o de Publicaci√≥n", ""))
    bd = _clean_field(row.get("Base de datos", ""))
    url = _clean_field(row.get("Url OA", "") or row.get("Url de acceso", ""))
    isbn = _clean_field(row.get("ISBN", ""))
    issn = _clean_field(row.get("ISSN1", ""))
    topog = _clean_field(row.get("No. Topogr√°fico", ""))

    partes: list[str] = []
    if aut:
        partes.append(f"{aut}.")
    if anio:
        partes.append(f"({anio}).")
    if tit:
        partes.append(f"{tit}.")
    if edit:
        partes.append(f"{edit}.")
    elif edit == "":
        partes.append("s.e.")

    acc: list[str] = []
    if bd:
        acc.append(f"Disponible en {bd}")
    if url:
        acc.append(url)
    if topog:
        acc.append(f"No. Topogr√°fico: {topog}")
    if acc:
        partes.append("; ".join(acc) + ".")

    extras: list[str] = []
    if isbn:
        extras.append(f"ISBN: {isbn}")
    if issn:
        extras.append(f"ISSN: {issn}")
    if extras:
        partes.append(" ".join(extras) + ".")

    return " ".join([p for p in partes if p]).replace("..", ".")


# --------- CARGA CACHEADA DE LAS BASES OFICIALES (COMPARTIDA ENTRE SESIONES) ----------
@st.cache_data(show_spinner=True)
def cargar_bd_digital_cache() -> pd.DataFrame:
    """
    Descarga y carga la BD de colecci√≥n Digital.
    Se ejecuta s√≥lo la primera vez en el servidor; luego se sirve desde cach√©.
    """
    resp = requests.get(URL_DIGITAL, headers=UA, timeout=600)
    resp.raise_for_status()
    bio = io.BytesIO(resp.content)
    df = pd.read_excel(bio, engine="openpyxl", dtype=str).fillna("")
    return df


@st.cache_data(show_spinner=True)
def cargar_bd_fisica_cache() -> pd.DataFrame:
    """
    Descarga y carga la BD de colecci√≥n F√≠sica.
    Se ejecuta s√≥lo la primera vez en el servidor; luego se sirve desde cach√©.
    """
    resp = requests.get(URL_FISICA, headers=UA, timeout=600)
    resp.raise_for_status()
    bio = io.BytesIO(resp.content)
    df = pd.read_excel(bio, engine="openpyxl", dtype=str).fillna("")
    return df


# CSS para cambiar el texto de "Browse files" (mejor esfuerzo)
st.markdown(
    """
<style>
button[title="Browse files"]{visibility: hidden;}
button[title="Browse files"]::after{
  content:" Cargar listado";
  visibility: visible;
  display:inline-block;
  padding:0.25rem 0.75rem;
  background:#2e7d32;
  color:white;
  border-radius:6px;
}
</style>
""",
    unsafe_allow_html=True,
)

# ---------------------------------- SIDEBAR ----------------------------------
with st.sidebar:
    st.image(LOGO_URL, use_container_width=True)
    st.caption(
        "Elaborado por David Camelo para la Biblioteca de la Universidad El Bosque"
    )

    # Bloques laterales s√≥lo cuando las bases ya est√°n listas
    if ss.bases_ready:
        if ss.metodo == "A":
            st.markdown("### Plantillas oficiales (M√©todo A)")
            st.markdown(f"- [Tem√°ticas]({URL_PLANTILLA_TEMATICAS})")
            st.markdown(f"- [T√©rminos a excluir]({URL_PLANTILLA_EXCLUSION})")

            st.markdown("### Archivos auxiliares (obligatorios)")
            bloqueados = ss.downloading  # mientras sincroniza, no subimos nada extra

            tem_up = st.file_uploader(
                "Tem√°ticas (.xlsx, col1=t√©rmino, col2=normalizado)",
                type=["xlsx"],
                key="tem_up_v82",
                disabled=bloqueados,
            )
            exc_up = st.file_uploader(
                "T√©rminos a excluir (.xlsx, col1)",
                type=["xlsx"],
                key="exc_up_v82",
                disabled=bloqueados,
            )

            if not bloqueados:
                if tem_up is not None:
                    df = safe_read_excel(tem_up, "Tem√°ticas")
                    ss.tematicas_df = df[[df.columns[0], df.columns[1]]].rename(
                        columns={
                            df.columns[0]: "termino",
                            df.columns[1]: "normalizado",
                        }
                    ).fillna("")
                    st.success(f"Tem√°ticas cargadas: {len(ss.tematicas_df)}")

                if exc_up is not None:
                    df = safe_read_excel(exc_up, "T√©rminos a excluir")
                    ss.excluir_df = df[[df.columns[0]]].rename(
                        columns={df.columns[0]: "excluir"}
                    ).fillna("")
                    st.success(f"T√©rminos a excluir cargados: {len(ss.excluir_df)}")
        else:
            st.markdown("### Instrucciones r√°pidas ‚Äì M√©todo B")
            st.markdown(
                """
1. Verifica que las bases est√©n sincronizadas (mensaje verde en la ventana principal).  
2. Elige colecciones (Digital/F√≠sica) y, si lo deseas, filtra por tipo de √≠tem normalizado.  
3. Define cada condici√≥n con:  
   - Operador booleano (primera, Y, O, NO)  
   - Campo (T√≠tulo, Autor(es), Tem√°ticas, etc.)  
   - Tipo de coincidencia (contiene la expresi√≥n, palabra completa, es igual a)  
   - Valor de b√∫squeda.  
4. Debes completar el **valor** en todas las condiciones definidas.  
5. Haz clic en **‚ÄúIniciar b√∫squeda avanzada (M√©todo B)‚Äù**.
                """
            )
    else:
        st.info(
            "Primero sincroniza las bases de datos oficiales desde la ventana principal "
            "para habilitar las opciones de b√∫squeda."
        )

    st.markdown("---")
    # Subir bases manualmente s√≥lo si a√∫n no se han cargado/sincronizado
    if not ss.bases_ready:
        with st.expander(
            "‚ûï Avanzado: subir bases Digital/F√≠sica manualmente", expanded=False
        ):
            up_dig = st.file_uploader(
                "Base de datos de la colecci√≥n Digital (.xlsx)",
                type=["xlsx"],
                key="up_dig_v82",
            )
            up_fis = st.file_uploader(
                "Base de datos de la colecci√≥n F√≠sica (.xlsx)",
                type=["xlsx"],
                key="up_fis_v82",
            )

            if up_dig is not None:
                ss.df_digital = safe_read_excel(up_dig, "Colecci√≥n Digital")
                st.success("Colecci√≥n Digital (manual) cargada.")
            if up_fis is not None:
                ss.df_fisica = safe_read_excel(up_fis, "Colecci√≥n F√≠sica")
                st.success("Colecci√≥n F√≠sica (manual) cargada.")
            if ss.df_digital is not None and ss.df_fisica is not None:
                ss.bases_ready = True
                st.success("‚úÖ Bases oficiales listas en memoria (carga manual).")

# ---------------------------------- CUERPO PRINCIPAL ----------------------------------
st.title("Herramienta para la elaboraci√≥n de bibliograf√≠as especializadas")

# --- Bloque de informaci√≥n general ---
with st.expander("‚ÑπÔ∏è Informaci√≥n general", expanded=True):
    st.markdown(
        f"""
- **Objetivo:** permitir la autogesti√≥n por programa/asignatura/tema y resaltar **t√©rminos a excluir** para depuraci√≥n manual.  
- Usa siempre las bases oficiales (Digital/F√≠sica) o s√∫belas **manualmente** desde la barra lateral.  
- **Plantillas:** [Tem√°ticas]({URL_PLANTILLA_TEMATICAS}) y [T√©rminos a excluir]({URL_PLANTILLA_EXCLUSION}).  
- Los archivos adjuntos **no se almacenan** por la Universidad y se eliminan al cerrar la app.  
- El proceso puede tardar algunos minutos; **puedes seguir usando tu equipo** (no cierres el navegador).
        """
    )

# --- Sincronizaci√≥n de bases ---
st.markdown("#### Bases de datos de las colecciones de la Biblioteca")

if not ss.bases_ready:
    st.info(
        "Antes de buscar, sincroniza las bases de datos oficiales o carga los archivos "
        "desde la barra lateral (opci√≥n **Avanzado**)."
    )

    mid_col = st.columns([1, 2, 1])[1]
    with mid_col:
        btn_sync = st.button(
            "üîÑ Sincronizar bases de datos oficiales",
            type="primary",
            use_container_width=True,
            disabled=ss.downloading,
        )

    if btn_sync and not ss.downloading:
        ss.downloading = True
        ss.descarga_disparada = True
        # Aqu√≠ se ver√° el spinner est√°ndar de Streamlit + "Running cargar_bd_..."
        with st.spinner(
            "Sincronizando colecciones **Digital** y **F√≠sica**‚Ä¶ "
            "Esta operaci√≥n se realiza s√≥lo una vez en el servidor y puede tardar varios minutos."
        ):
            try:
                ss.df_digital = cargar_bd_digital_cache()
                ss.df_fisica = cargar_bd_fisica_cache()
                ss.bases_ready = True
                st.success("‚úÖ Bases oficiales listas en memoria.")
            except Exception as e:
                st.error(f"No fue posible sincronizar las bases oficiales: {e}")
                ss.bases_ready = False
        ss.downloading = False

if not ss.bases_ready:
    st.stop()
else:
    st.success("‚úÖ Bases oficiales listas en memoria (sesi√≥n).")


# ---------------------------------- SELECCI√ìN DE M√âTODO ----------------------------------
st.markdown("### Selecciona el modo de b√∫squeda")

prev_metodo = ss.metodo
metodo_label = st.radio(
    "Modo de b√∫squeda",
    (
        "M√©todo A ‚Äì listado de tem√°ticas (plantilla)",
        "M√©todo B ‚Äì b√∫squeda avanzada tipo descubridor (experimental)",
    ),
    index=0 if ss.metodo == "A" else 1,
)
new_metodo = "A" if metodo_label.startswith("M√©todo A") else "B"

# Si el usuario cambia de m√©todo, limpiamos estado de b√∫squeda (como si fuera "Nueva b√∫squeda")
if new_metodo != prev_metodo:
    for k in (
        "tematicas_df",
        "excluir_df",
        "results_df",
        "bitacora_df",
        "b_conds",
    ):
        ss[k] = None if k != "b_conds" else []
    ss.b_num_cond = 2

ss.metodo = new_metodo

# --- Paso a paso seg√∫n m√©todo ---
if ss.metodo == "A":
    with st.expander("üß≠ Paso a paso ‚Äì M√©todo A (listado de tem√°ticas)", expanded=True):
        st.markdown(
            f"""
**1) Sincronizaci√≥n (obligatoria una sola vez en el servidor).**  
Si a√∫n no lo has hecho, usa **‚ÄúSincronizar bases de datos oficiales‚Äù** o carga las bases desde *Avanzado* en la barra lateral.

**2) Cargue sus tem√°ticas.**  
Descargue la plantilla de [Tem√°ticas]({URL_PLANTILLA_TEMATICAS}).  
La **columna 1** incluye variaciones del t√©rmino (con/sin tildes, otros idiomas).  
La **columna 2** agrupa/normaliza el t√©rmino, que ser√° el que ver√°s en los resultados.

**3) Cargue t√©rminos a excluir.**  
Use la plantilla de [T√©rminos a excluir]({URL_PLANTILLA_EXCLUSION}).  
Sirve para evitar falsos positivos (p. ej., buscar ‚Äúecolog√≠a‚Äù sin recuperar ‚Äúginecolog√≠a‚Äù).

**4) Par√°metros de b√∫squeda.**  
Por defecto la b√∫squeda se hace en **T√≠tulo** y **Tem√°ticas**, y se eliminan duplicados por **Url OA** (Digital) y **No. Topogr√°fico** (F√≠sica).  
Puedes cambiar estas columnas en la secci√≥n **Configuraci√≥n de b√∫squeda y duplicados**.

**5) Ejecute la b√∫squeda.**  
Pulse **‚ÄúüöÄ Iniciar b√∫squeda (M√©todo A)‚Äù**. Ver√° una tabla con los resultados (vista de hasta 200 filas por defecto).  
Podr√° **filtrar**, **marcar filas** y **exportar** en CSV/XLSX o **citas APA** para los t√≠tulos seleccionados.

**6) Exportaciones y bit√°cora.**  
El Excel incluye la **bit√°cora por t√©rmino** y resalta coincidencias con **t√©rminos a excluir**.  
Las exportaciones ‚Äúsolo seleccionados‚Äù respetan lo marcado en la tabla.

**7) Nueva b√∫squeda.**  
Pulse **‚ÄúNueva b√∫squeda‚Äù** para cargar otras tem√°ticas y t√©rminos **sin re-sincronizar** las bases.
Al cerrar la pesta√±a, la sesi√≥n se pierde (no se guarda nada).
            """
        )
else:
    st.markdown(
        "‚ÑπÔ∏è Est√°s usando el **M√©todo B** (b√∫squeda avanzada tipo descubridor). "
        "Las instrucciones r√°pidas est√°n en la barra lateral izquierda."
    )

# ---------------------------------- NUEVA B√öSQUEDA ----------------------------------
col_nb = st.columns([1, 1, 4])[0]
with col_nb:
    if st.button("üß™ Nueva b√∫squeda", use_container_width=True):
        for k in (
            "tematicas_df",
            "excluir_df",
            "results_df",
            "bitacora_df",
            "b_conds",
        ):
            ss[k] = None if k != "b_conds" else []
        ss.b_num_cond = 2
        st.toast("Listo. Carga nuevos t√©rminos o ajusta las condiciones para buscar de nuevo.")

# ==========================================================================================
# M√âTODO A ‚Äì LISTADO DE TEM√ÅTICAS
# ==========================================================================================
def ejecutar_busqueda_metodo_a(col_busq1: str, col_busq2: str, col_dup_dig: str, col_dup_fis: str):  # noqa: E501
    if ss.tematicas_df is None or ss.excluir_df is None:
        st.warning(
            "Para usar el **M√©todo A** debes cargar **Tem√°ticas** y **T√©rminos a excluir** "
            "en la barra lateral."
        )
        return

    barra = st.progress(0)
    estado = st.empty()

    DF_D = ss.df_digital.copy()
    DF_F = ss.df_fisica.copy()

    # Asegurar columnas como texto
    _prepara_columnas(DF_D, [col_busq1, col_busq2, col_dup_dig])
    _prepara_columnas(DF_F, [col_busq1, col_busq2, col_dup_fis])

    def _buscar(
        df: pd.DataFrame,
        fuente: str,
        tem_df: pd.DataFrame,
        offset: int,
        total_steps: int,
    ) -> pd.DataFrame:
        res_list: list[pd.DataFrame] = []
        tem = tem_df.copy()
        tem["termino"] = tem["termino"].astype(str).fillna("")
        tem["normalizado"] = tem["normalizado"].astype(str).fillna("")
        N = len(tem)
        t0 = time.time()

        for i, row in tem.iterrows():
            term = normalize_text(row["termino"])
            if term:
                m1 = df[col_busq1].map(
                    lambda s: term in normalize_text(s)
                )
                m2 = df[col_busq2].map(
                    lambda s: term in normalize_text(s)
                )
                md = df[m1 | m2].copy()
                if not md.empty:
                    md["Tem√°tica"] = row["termino"]
                    md["Tem√°tica normalizada"] = row["normalizado"]
                    md["Columna de coincidencia"] = None
                    md.loc[m1[m1].index, "Columna de coincidencia"] = col_busq1
                    md.loc[m2[m2].index, "Columna de coincidencia"] = md[
                        "Columna de coincidencia"
                    ].fillna(col_busq2)
                    md["Fuente"] = fuente
                    res_list.append(md)

            frac = (i + 1) / max(N, 1)
            elapsed = time.time() - t0
            est_total = elapsed / max(frac, 1e-6)
            est_rem = max(0, int(est_total - elapsed))
            barra.progress(min(1.0, (offset + i + 1) / total_steps))
            estado.info(
                f"{fuente}: {i+1}/{N} t√©rminos ‚Ä¢ transcurrido: {int(elapsed)} s ‚Ä¢ restante: {est_rem} s"  # noqa: E501
            )

        if res_list:
            return pd.concat(res_list, ignore_index=True)
        return pd.DataFrame()

    total = len(ss.tematicas_df) * 2
    res_d = _buscar(
        DF_D,
        "Digital",
        ss.tematicas_df,
        offset=0,
        total_steps=total,
    )
    res_f = _buscar(
        DF_F,
        "F√≠sica",
        ss.tematicas_df,
        offset=len(ss.tematicas_df),
        total_steps=total,
    )

    if not res_d.empty and col_dup_dig in res_d.columns:
        res_d = res_d.drop_duplicates(subset=[col_dup_dig], keep="first")
    if not res_f.empty and col_dup_fis in res_f.columns:
        res_f = res_f.drop_duplicates(subset=[col_dup_fis], keep="first")

    res = (
        pd.concat([res_d, res_f], ignore_index=True)
        if not (res_d.empty and res_f.empty)
        else pd.DataFrame()
    )

    ss.results_df = res

    # --- bit√°cora por t√©rmino ---
    tem = (
        ss.tematicas_df[["termino", "normalizado"]]
        .drop_duplicates()
        .reset_index(drop=True)
    )
    fuentes = pd.DataFrame({"Fuente": ["Digital", "F√≠sica"]})
    grid = fuentes.assign(key=1).merge(
        tem.assign(key=1), on="key"
    ).drop("key", axis=1)

    if res.empty:
        counts = pd.DataFrame(
            columns=["Fuente", "Tem√°tica", "Tem√°tica normalizada", "Resultados"]
        )
    else:
        counts = (
            res.groupby(["Fuente", "Tem√°tica", "Tem√°tica normalizada"], dropna=False)
            .size()
            .reset_index(name="Resultados")
        )

    bit = (
        grid.merge(
            counts,
            how="left",
            left_on=["Fuente", "termino", "normalizado"],
            right_on=["Fuente", "Tem√°tica", "Tem√°tica normalizada"],
        )
        .drop(columns=["Tem√°tica", "Tem√°tica normalizada"], errors="ignore")
        .rename(columns={"termino": "T√©rmino", "normalizado": "Normalizado"})
    )

    bit["Resultados"] = bit["Resultados"].fillna(0).astype(int)
    bit = bit.sort_values(
        ["Fuente", "Resultados", "T√©rmino"], ascending=[True, False, True]
    ).reset_index(drop=True)
    ss.bitacora_df = bit

    barra.progress(1.0)
    estado.empty()
    st.success("B√∫squeda finalizada (M√©todo A).")


# ==========================================================================================
# M√âTODO B ‚Äì B√öSQUEDA AVANZADA
# ==========================================================================================
CAMPOS_B = {
    "Cualquier campo": None,
    "T√≠tulo": "T√≠tulo",
    "Autor(es)": "Autor(es)",
    "Tem√°ticas": "Tem√°ticas",
    "Editorial": "Editorial",
    "A√±o de Publicaci√≥n": "A√±o de Publicaci√≥n",
}

# Tipo de coincidencia sobre el campo de texto
OPERADORES_B = [
    "Contiene la expresi√≥n",
    "Palabra completa",
    "Es igual a",
]

# Operadores booleanos (entre condiciones)
CONECTORES_B = ["(primera)", "Y (AND)", "O (OR)", "NO (NOT)"]


def _mask_condicion(base: pd.DataFrame, campo: str | None, operador: str, valor: str):
    """
    Genera una m√°scara booleana para una condici√≥n simple sobre `base`.
    `operador` indica el tipo de coincidencia de texto (no el operador booleano).
    """
    val_norm = normalize_text(valor).lower()

    def _match_series(series: pd.Series) -> pd.Series:
        series = series.fillna("").astype(str)

        def _norm(s: str) -> str:
            return normalize_text(s).lower()

        if operador == "Contiene la expresi√≥n":
            return series.map(lambda x: val_norm in _norm(x))
        elif operador == "Palabra completa":
            # Coincidencia por palabra completa (tokens alfanum√©ricos)
            return series.map(
                lambda x: val_norm
                in re.findall(r"\w+", _norm(x), flags=re.UNICODE)
            )
        elif operador == "Es igual a":
            # Coincidencia exacta del texto completo normalizado
            return series.map(lambda x: _norm(x) == val_norm)
        else:
            # Por defecto, nos comportamos como "Contiene la expresi√≥n"
            return series.map(lambda x: val_norm in _norm(x))

    if campo is None:
        series_list = []
        for c in [
            "T√≠tulo",
            "Autor(es)",
            "Autor(es) ",
            "Tem√°ticas",
            "Editorial",
            "Base de datos",
        ]:
            if c in base.columns:
                series_list.append(_match_series(base[c]))
        if not series_list:
            return pd.Series(False, index=base.index)
        mask = series_list[0]
        for s in series_list[1:]:
            mask = mask | s
        return mask
    else:
        if campo not in base.columns:
            return pd.Series(False, index=base.index)
        return _match_series(base[campo])


def ejecutar_busqueda_metodo_b(
    colecciones: list[str],
    tipos_sel: list[str],
    condiciones: List[Dict[str, Any]],
):
    # Filtrar condiciones con valor no vac√≠o (protecci√≥n adicional)
    condiciones = [
        c for c in condiciones if c.get("valor", "").strip()
    ]
    if not condiciones:
        st.warning(
            "Debes indicar al menos un valor de b√∫squeda en las condiciones antes de ejecutar "
            "la b√∫squeda avanzada."
        )
        return

    # Construir tabla base Digital + F√≠sica
    DF_D = ss.df_digital.copy()
    DF_F = ss.df_fisica.copy()
    DF_D["Fuente"] = "Digital"
    DF_F["Fuente"] = "F√≠sica"
    base = pd.concat([DF_D, DF_F], ignore_index=True)

    if colecciones:
        base = base[base["Fuente"].isin(colecciones)]

    if tipos_sel and TIPO_NORMAL_COL in base.columns:
        base = base[base[TIPO_NORMAL_COL].isin(tipos_sel)]

    # Nos aseguramos que todas las columnas relevantes sean texto
    _prepara_columnas(
        base,
        [
            "T√≠tulo",
            "Autor(es)",
            "Autor(es) ",
            "Tem√°ticas",
            "Editorial",
            "Base de datos",
            "A√±o de Publicaci√≥n",
        ],
    )

    res = None
    for idx, cond in enumerate(condiciones):
        valor = cond.get("valor", "").strip()
        if not valor:
            continue

        campo_key = cond.get("campo", "Cualquier campo")
        campo_col = CAMPOS_B.get(campo_key)

        # Ajuste especial para Autor(es): puede ser "Autor(es)" o "Autor(es) "
        if campo_key == "Autor(es)":
            if "Autor(es)" in base.columns:
                campo_col = "Autor(es)"
            elif "Autor(es) " in base.columns:
                campo_col = "Autor(es) "
            else:
                campo_col = None

        operador = cond.get("operador", "Contiene la expresi√≥n")
        conector = cond.get("conector", "(primera)")

        mask = _mask_condicion(base, campo_col, operador, valor)

        # Primera condici√≥n
        if res is None:
            if conector.startswith("NO"):
                res = base[~mask].copy()
            else:
                res = base[mask].copy()
            continue

        # Resto de condiciones
        if conector.startswith("Y"):
            submask = mask.loc[res.index]
            res = res[submask].copy()
        elif conector.startswith("O"):
            nuevos = base[mask].copy()
            res = (
                pd.concat([res, nuevos], ignore_index=True)
                .drop_duplicates()
                .reset_index(drop=True)
            )
        elif conector.startswith("NO"):
            submask = mask.loc[res.index]
            res = res[~submask].copy()
        else:
            res = base[mask].copy()

    if res is None:
        st.warning(
            "No se encontraron resultados con las condiciones especificadas. "
            "Revisa los t√©rminos de b√∫squeda."
        )
        return

    ss.results_df = res
    ss.bitacora_df = None
    st.success(f"B√∫squeda avanzada finalizada. Resultados: {len(res):,}")


# ==========================================================================================
# RENDERIZADO COM√öN DE RESULTADOS + EXPORTACIONES
# ==========================================================================================
def render_resultados(con_bitacora: bool):
    st.subheader("Resultados")

    if ss.results_df is None or ss.results_df.empty:
        st.info("A√∫n no hay resultados. Ejecuta una b√∫squeda.")
        return

    res = ss.results_df.copy()

    # Ocultar columnas internas: Unnamed*, sin nombre, Prioridad B√∫squeda
    cols_to_hide = [
        c for c in res.columns if c.startswith("Unnamed") or not str(c).strip()
    ]
    if "Prioridad B√∫squeda" in res.columns:
        cols_to_hide.append("Prioridad B√∫squeda")
    if cols_to_hide:
        res = res.drop(columns=list(dict.fromkeys(cols_to_hide)), errors="ignore")

    # Filtros r√°pidos
    if ss.metodo == "A":
        colf1, colf2, colf3 = st.columns([1, 1, 2])
    else:
        colf1, colf3 = st.columns([1, 2])
        colf2 = None

    with colf1:
        filtro_fuente = st.multiselect(
            "Fuente",
            options=sorted(res["Fuente"].dropna().unique().tolist())
            if "Fuente" in res.columns
            else [],
            default=None,
        )

    filtro_tema = None
    if ss.metodo == "A" and colf2 is not None:
        with colf2:
            col_tema_norm = "Tem√°tica normalizada"
            temas_norm = (
                sorted(res[col_tema_norm].dropna().unique().tolist())
                if col_tema_norm in res.columns
                else []
            )
            filtro_tema = st.multiselect(
                "Tem√°tica normalizada", options=temas_norm, default=None
            )

    with colf3:
        tipo_opts = (
            sorted(
                res.get(TIPO_NORMAL_COL, pd.Series(dtype=str))
                .dropna()
                .unique()
                .tolist()
            )
            if TIPO_NORMAL_COL in res.columns
            else []
        )
        filtro_tipo = st.multiselect(
            "Tipo normalizado", options=tipo_opts, default=None
        )

    if filtro_fuente:
        res = res[res["Fuente"].isin(filtro_fuente)]
    if filtro_tema and "Tem√°tica normalizada" in res.columns:
        res = res[res["Tem√°tica normalizada"].isin(filtro_tema)]
    if filtro_tipo and TIPO_NORMAL_COL in res.columns:
        res = res[res[TIPO_NORMAL_COL].isin(filtro_tipo)]

    st.caption(f"Filas totales (despu√©s de filtros): **{len(res):,}**")

    # Columna de selecci√≥n
    res_view = res.copy()
    if "__Seleccionar__" not in res_view.columns:
        res_view.insert(0, "__Seleccionar__", False)

    cva, cvb = st.columns([1, 1])
    with cva:
        show_all = st.checkbox("Mostrar todas las filas (Vista)", value=False)
    with cvb:
        limit = st.number_input(
            "Filas a mostrar (Vista)", min_value=50, max_value=10000, value=200, step=50
        )

    res_view_show = res_view if show_all else res_view.head(int(limit))

    res_view_show = st.data_editor(
        res_view_show,
        use_container_width=True,
        height=520,
        column_config={
            "__Seleccionar__": st.column_config.CheckboxColumn("Seleccionar"),
        },
        disabled=[c for c in res_view_show.columns if c != "__Seleccionar__"],
        key=f"data_editor_res_{ss.metodo}",
    )

    seleccion_mask = (
        res_view_show["__Seleccionar__"]
        if "__Seleccionar__" in res_view_show.columns
        else pd.Series(False, index=res_view_show.index)
    )
    seleccionados = res_view_show[seleccion_mask].drop(
        columns=["__Seleccionar__"], errors="ignore"
    )
    st.caption(f"Seleccionados en la vista: **{len(seleccionados):,}**")

    # ---------------------------------- Exportaciones ----------------------------------
    st.markdown("##### Exportaciones")
    colx1, colx2, colx3, colx4, colx5 = st.columns([1.2, 1.2, 1.6, 1.6, 2])

    # CSV completo (filtrado)
    with colx1:
        st.download_button(
            "‚¨áÔ∏è CSV (todo lo filtrado)",
            data=_prep_export(res).to_csv(index=False).encode("utf-8"),
            file_name="resultados_filtrados.csv",
            mime="text/csv",
            use_container_width=True,
        )

    # CSV de seleccionados
    with colx2:
        st.download_button(
            "‚¨áÔ∏è CSV (solo seleccionados)",
            data=_prep_export(
                seleccionados if not seleccionados.empty else res.head(0)
            )
            .to_csv(index=False)
            .encode("utf-8"),
            file_name="resultados_seleccionados.csv",
            mime="text/csv",
            disabled=seleccionados.empty,
            use_container_width=True,
        )

    # Excel completo
    with colx3:
        import xlsxwriter  # noqa: F401

        xbio = io.BytesIO()
        writer = pd.ExcelWriter(xbio, engine="xlsxwriter")

        res_x = _prep_export(res)
        res_x.to_excel(writer, index=False, sheet_name="Resultados")

        if con_bitacora:
            # Resaltado por t√©rminos a excluir (s√≥lo M√©todo A)
            if ss.excluir_df is not None:
                wb = writer.book
                ws = writer.sheets["Resultados"]
                fmt = wb.add_format({"bg_color": "#FFF599"})

                cols = list(res_x.columns)
                col_tit_name = (
                    EXPORT_RENAME.get(DEFAULT_COL_TITULO, DEFAULT_COL_TITULO)
                )
                col_tem_name = (
                    EXPORT_RENAME.get(DEFAULT_COL_TEMATICAS, DEFAULT_COL_TEMATICAS)
                )
                col_tit = cols.index(col_tit_name) + 1 if col_tit_name in cols else None
                col_tem = cols.index(col_tem_name) + 1 if col_tem_name in cols else None

                excluye = [
                    normalize_text(x)
                    for x in ss.excluir_df["excluir"].astype(str).tolist()
                    if str(x).strip()
                ]

                for r in range(1, len(res_x) + 1):
                    if col_tit is not None:
                        v = normalize_text(res_x.iloc[r - 1, col_tit - 1])
                        if any(t in v for t in excluye):
                            ws.write(r, col_tit - 1, res_x.iloc[r - 1, col_tit - 1], fmt)
                    if col_tem is not None:
                        v = normalize_text(res_x.iloc[r - 1, col_tem - 1])
                        if any(t in v for t in excluye):
                            ws.write(r, col_tem - 1, res_x.iloc[r - 1, col_tem - 1], fmt)

            if ss.bitacora_df is not None:
                ss.bitacora_df.to_excel(writer, index=False, sheet_name="Bit√°cora")

            writer.close()
            xbio.seek(0)
            st.download_button(
                "‚¨áÔ∏è Excel (filtrado + resaltado + Bit√°cora)",
                data=xbio.getvalue(),
                file_name="resultados_filtrados.xlsx",
                mime=(
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                ),
                use_container_width=True,
            )
        else:
            writer.close()
            xbio.seek(0)
            st.download_button(
                "‚¨áÔ∏è Excel (todo lo filtrado)",
                data=xbio.getvalue(),
                file_name="resultados_filtrados.xlsx",
                mime=(
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                ),
                use_container_width=True,
            )

    # Excel de seleccionados
    with colx4:
        if not seleccionados.empty:
            sel_x = _prep_export(seleccionados)
            bio_sel = io.BytesIO()
            with pd.ExcelWriter(bio_sel, engine="xlsxwriter") as wsel:
                sel_x.to_excel(wsel, index=False, sheet_name="Seleccionados")
            bio_sel.seek(0)
            st.download_button(
                "‚¨áÔ∏è Excel (solo seleccionados)",
                data=bio_sel.getvalue(),
                file_name="resultados_seleccionados.xlsx",
                mime=(
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                ),
                use_container_width=True,
            )
        else:
            st.download_button(
                "‚¨áÔ∏è Excel (solo seleccionados)",
                data=b"",
                file_name="resultados_seleccionados.xlsx",
                disabled=True,
                use_container_width=True,
            )

    # Citas APA para seleccionados
    with colx5:
        if not seleccionados.empty:
            citas = [build_apa(r) for _, r in seleccionados.iterrows()]
            txt = "\n\n".join(c for c in citas if c.strip())
            st.download_button(
                "üßæ Citas APA (seleccionados)",
                data=txt.encode("utf-8"),
                file_name="citas_apa.txt",
                mime="text/plain",
                use_container_width=True,
            )
        else:
            st.download_button(
                "üßæ Citas APA (seleccionados)",
                data=b"",
                file_name="citas_apa.txt",
                mime="text/plain",
                use_container_width=True,
                disabled=True,
            )


# ==========================================================================================
# L√ìGICA PRINCIPAL POR M√âTODO
# ==========================================================================================
if ss.metodo == "A":
    # --- Validaciones M√©todo A ---
    if ss.tematicas_df is None or ss.excluir_df is None:
        st.warning(
            "Para usar el **M√©todo A** debes cargar **Tem√°ticas** y **T√©rminos a excluir** "
            "en la barra lateral."
        )
    else:
        st.subheader("Configuraci√≥n de b√∫squeda y duplicados (M√©todo A)")

        cols_dig = list(ss.df_digital.columns)
        cols_fis = list(ss.df_fisica.columns)
        common_cols = sorted(set(cols_dig + cols_fis))

        c1, c2, c3, c4 = st.columns([1, 1, 1, 1])

        with c1:
            col_busq1 = st.selectbox(
                "B√∫squeda principal por",
                options=common_cols,
                index=get_index_or_first(common_cols, DEFAULT_COL_TITULO),
                key="col_busq1_v82",
            )

        with c2:
            col_busq2 = st.selectbox(
                "B√∫squeda complementaria por",
                options=common_cols,
                index=get_index_or_first(common_cols, DEFAULT_COL_TEMATICAS),
                key="col_busq2_v82",
            )

        with c3:
            col_dup_dig = st.selectbox(
                "Columna de duplicados en Colecci√≥n Digital",
                options=cols_dig,
                index=get_index_or_first(cols_dig, DEFAULT_DUP_DIGITAL),
                key="dup_dig_v82",
            )

        with c4:
            col_dup_fis = st.selectbox(
                "Columna de duplicados en Colecci√≥n F√≠sica",
                options=cols_fis,
                index=get_index_or_first(cols_fis, DEFAULT_DUP_FISICA),
                key="dup_fis_v82",
            )

        st.caption(
            "Por defecto se usan ‚ÄúT√≠tulo‚Äù y ‚ÄúTem√°ticas‚Äù, y duplicados por "
            "‚ÄúUrl OA‚Äù / ‚ÄúNo. Topogr√°fico‚Äù. Puedes cambiarlos si lo necesitas."
        )

        st.markdown("---")

        if st.button(
            "üöÄ Iniciar b√∫squeda (M√©todo A)",
            type="primary",
            use_container_width=True,
        ):
            try:
                ejecutar_busqueda_metodo_a(
                    col_busq1=col_busq1,
                    col_busq2=col_busq2,
                    col_dup_dig=col_dup_dig,
                    col_dup_fis=col_dup_fis,
                )
            except Exception as e:
                st.error(f"Ocurri√≥ un problema durante la b√∫squeda: {e}")

        # Mostrar resultados y bit√°cora
        render_resultados(con_bitacora=True)

        st.subheader("üìë Bit√°cora por t√©rmino")
        if ss.bitacora_df is None or ss.bitacora_df.empty:
            st.info("A√∫n no hay bit√°cora. Ejecuta la b√∫squeda del M√©todo A.")
        else:
            st.dataframe(ss.bitacora_df, use_container_width=True, height=360)
            st.download_button(
                "Descargar bit√°cora (.csv)",
                data=ss.bitacora_df.to_csv(index=False).encode("utf-8"),
                file_name="bitacora_por_termino.csv",
                mime="text/csv",
                use_container_width=True,
            )

else:
    # ======================= M√âTODO B ==========================
    st.subheader("B√∫squeda avanzada (M√©todo B ‚Äì experimental)")

    # Alcance de b√∫squeda
    colc1, colc2 = st.columns([1, 1])
    with colc1:
        colecciones = st.multiselect(
            "Colecciones a incluir",
            options=["Digital", "F√≠sica"],
            default=["Digital", "F√≠sica"],
        )
    with colc2:
        # tipos normalizados disponibles
        todos_tipos = sorted(
            pd.concat([ss.df_digital, ss.df_fisica], ignore_index=True)
            .get(TIPO_NORMAL_COL, pd.Series(dtype=str))
            .dropna()
            .unique()
            .tolist()
        )
        tipos_sel = st.multiselect(
            "Tipo de √≠tem normalizado",
            options=todos_tipos,
            default=todos_tipos,  # por defecto TODOS
        )

    st.markdown(
        "Define una o varias condiciones. Se aplican en orden y se combinan con **Y (AND)**, "
        "**O (OR)** o **NO (NOT)**."
    )

    # N√∫mero de condiciones
    ss.b_num_cond = int(
        st.number_input(
            "N√∫mero de condiciones",
            min_value=1,
            max_value=5,
            value=ss.b_num_cond or 1,
            step=1,
        )
    )

    # Asegurar lista de condiciones en estado
    conds: List[Dict[str, Any]] = ss.b_conds or []
    while len(conds) < ss.b_num_cond:
        conds.append(
            {
                "conector": "(primera)" if len(conds) == 0 else "Y (AND)",
                "campo": "T√≠tulo",
                "operador": "Contiene la expresi√≥n",
                "valor": "",
            }
        )
    if len(conds) > ss.b_num_cond:
        conds = conds[: ss.b_num_cond]

    # Render de cada condici√≥n
    for i in range(ss.b_num_cond):
        st.markdown(f"**Condici√≥n {i+1}**")
        c1, c2, c3, c4 = st.columns([1, 1, 1, 3])

        with c1:
            opciones_con = CONECTORES_B if i > 0 else ["(primera)"]
            conds[i]["conector"] = st.selectbox(
                "Operador booleano",
                options=opciones_con,
                index=opciones_con.index(conds[i]["conector"])
                if conds[i]["conector"] in opciones_con
                else 0,
                key=f"b_con_{i}",
            )
        with c2:
            conds[i]["campo"] = st.selectbox(
                "Campo",
                options=list(CAMPOS_B.keys()),
                index=list(CAMPOS_B.keys()).index(conds[i]["campo"])
                if conds[i]["campo"] in CAMPOS_B
                else 0,
                key=f"b_cam_{i}",
            )
        with c3:
            conds[i]["operador"] = st.selectbox(
                "Tipo de coincidencia",
                options=OPERADORES_B,
                index=OPERADORES_B.index(conds[i]["operador"])
                if conds[i]["operador"] in OPERADORES_B
                else 0,
                key=f"b_op_{i}",
            )
        with c4:
            conds[i]["valor"] = st.text_input(
                "Valor",
                value=conds[i]["valor"],
                key=f"b_val_{i}",
            )

        st.markdown("---")

    ss.b_conds = conds

    if st.button(
        "üöÄ Iniciar b√∫squeda avanzada (M√©todo B)",
        type="primary",
        use_container_width=True,
    ):
        # Validaci√≥n: no permitir condiciones en blanco
        valores = [c.get("valor", "").strip() for c in conds]
        if any(v == "" for v in valores):
            st.warning(
                "Para ejecutar la b√∫squeda avanzada debes indicar un **valor de b√∫squeda** "
                "en todas las condiciones definidas. "
                "Si no vas a usar alguna condici√≥n, reduce el n√∫mero en "
                "‚ÄúN√∫mero de condiciones‚Äù."
            )
        else:
            try:
                ejecutar_busqueda_metodo_b(
                    colecciones=colecciones,
                    tipos_sel=tipos_sel,
                    condiciones=conds,
                )
            except Exception as e:
                st.error(f"Ocurri√≥ un problema durante la b√∫squeda avanzada: {e}")

    # Resultados sin bit√°cora ni resaltado especial
    render_resultados(con_bitacora=False)
