# -*- coding: utf-8 -*-
# Herramienta para la elaboraci√≥n de bibliograf√≠as especializadas
# v8.2 ‚Äì M√©todos A/B, UI refinada, instrucciones por m√©todo

import io
import os
import time
import tempfile
from typing import List, Dict, Any

import pandas as pd
import requests
import streamlit as st

# ---------------------------------- CONFIGURACI√ìN B√ÅSICA ----------------------------------
st.set_page_config(page_title="Herramienta de bibliograf√≠as", layout="wide")

LOGO_URL = "https://biblioteca.unbosque.edu.co/sites/default/files/Logos/Logo%201%20Blanco.png"

# URLs oficiales (Digital/F√≠sica y plantillas)
URL_DIGITAL = "https://biblioteca.unbosque.edu.co/sites/default/files/Formatos-Biblioteca/Biblioteca%20Colecci%C3%B3n%20Digital.xlsx"
URL_FISICA = "https://biblioteca.unbosque.edu.co/sites/default/files/Formatos-Biblioteca/Biblioteca%20BD%20Colecci%C3%B3n%20F%C3%ADsica.xlsx"

URL_PLANTILLA_TEMATICAS = "https://biblioteca.unbosque.edu.co/sites/default/files/Formatos-Biblioteca/Plantilla%20Tem%C3%A1ticas.xlsx"
URL_PLANTILLA_EXCLUSION = "https://biblioteca.unbosque.edu.co/sites/default/files/Formatos-Biblioteca/Plantilla%20T%C3%A9rminos%20a%20excluir.xlsx"

DEFAULT_COL_TITULO = "T√≠tulo"
DEFAULT_COL_TEMATICAS = "Tem√°ticas"
DEFAULT_DUP_DIGITAL = "Url OA"
DEFAULT_DUP_FISICA = "No. Topogr√°fico"

UA = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari"  # noqa: E501
}

# Columnas a OMITIR en exportaciones CSV/XLSX
EXPORT_DROP_COLS = {
    "Fecha de actualizaci√≥n",
    "Tipo de √≠tem normalizado mat especial",
    "Formato",
    "Prioridad B√∫squeda",
}

# Renombres para exportaci√≥n
EXPORT_RENAME = {
    "Tem√°ticas": "Tem√°ticas catalogadas por el Editor",
    "Tem√°tica": "T√©rmino de b√∫squeda",
    "Tem√°tica normalizada": "T√©rmino de b√∫squeda normalizado",
    "Url en LOCATE/IDEA": "Url de acceso",
}

TIPO_NORMAL_COL = "Tipo de √≠tem normalizado mat especial"

# ---------------------------------- ESTADO GLOBAL ----------------------------------
ss = st.session_state

ss.setdefault("df_digital", None)
ss.setdefault("df_fisica", None)
ss.setdefault("bases_ready", False)

ss.setdefault("downloading", False)
ss.setdefault("descarga_disparada", False)

# Insumos m√©todo A
ss.setdefault("tematicas_df", None)
ss.setdefault("excluir_df", None)

# Resultados comunes (A y B)
ss.setdefault("results_df", None)
ss.setdefault("bitacora_df", None)

# M√©todo de b√∫squeda actual: "A" (tem√°ticas) o "B" (avanzada)
ss.setdefault("metodo", "A")

# Estado m√©todo B
ss.setdefault("b_num_cond", 2)
ss.setdefault("b_conds", [])  # lista de dicts con los par√°metros de cada condici√≥n


# ---------------------------------- UTILIDADES ----------------------------------
def normalize_text(s: Any) -> str:
    if pd.isna(s):
        return ""
    s = str(s)
    return (
        s.replace("\u0301", "")
        .replace("\u0303", "")
        .replace("\u2019", "'")
        .replace("\xa0", " ")
        .strip()
    )


def _head_content_length(url: str, timeout: int = 30) -> int | None:
    try:
        r = requests.head(url, allow_redirects=True, timeout=timeout, headers=UA)
        r.raise_for_status()
        cl = r.headers.get("Content-Length")
        return int(cl) if cl is not None else None
    except Exception:
        return None


def download_with_resume(
    url: str,
    label: str,
    container=None,
    max_retries: int = 5,
    chunk_size: int = 256 * 1024,
    timeout: int = 300,
) -> io.BytesIO:
    """
    Descarga con barra de progreso y reintentos. Devuelve BytesIO.
    """
    where = container if container is not None else st
    status = where.empty()
    bar = where.progress(0)
    info = where.empty()

    tmp_dir = tempfile.gettempdir()
    tmp_path = os.path.join(tmp_dir, f"dl_{abs(hash(url))}.part")

    total_size = _head_content_length(url)
    attempt = 0

    while attempt < max_retries:
        attempt += 1
        try:
            downloaded = os.path.getsize(tmp_path) if os.path.exists(tmp_path) else 0
            headers = dict(UA)
            mode = "wb"

            if downloaded and total_size and downloaded < total_size:
                headers["Range"] = f"bytes={downloaded}-"
                mode = "ab"

            status.info(f"Descargando {label}‚Ä¶ (intento {attempt}/{max_retries})")

            with requests.get(
                url,
                stream=True,
                headers=headers,
                timeout=timeout,
                allow_redirects=True,
            ) as r:
                if headers.get("Range") and r.status_code == 200:
                    if os.path.exists(tmp_path):
                        os.remove(tmp_path)
                    downloaded = 0
                    mode = "wb"

                r.raise_for_status()

                content_length = r.headers.get("Content-Length")
                expected_total = downloaded + int(content_length) if content_length else total_size  # noqa: E501

                last = time.time()
                with open(tmp_path, mode) as f:
                    for chunk in r.iter_content(chunk_size=chunk_size):
                        if not chunk:
                            continue
                        f.write(chunk)
                        downloaded += len(chunk)
                        if expected_total and time.time() - last > 0.1:
                            bar.progress(min(1.0, downloaded / expected_total))
                            info.write(
                                f"{downloaded/1e6:,.1f} MB / {expected_total/1e6:,.1f} MB"
                            )
                            last = time.time()

            if total_size and downloaded < total_size:
                raise requests.exceptions.ChunkedEncodingError(
                    f"Descarga incompleta: {downloaded} de {total_size} bytes"
                )

            bar.progress(1.0)
            status.success(f"{label} descargado correctamente.")
            info.empty()
            bar.empty()
            status.empty()

            with open(tmp_path, "rb") as f:
                data = f.read()
            return io.BytesIO(data)

        except Exception as e:
            info.empty()
            bar.empty()
            status.warning(f"Fallo al descargar {label}: {e}")
            if attempt < max_retries:
                time.sleep(2)
            else:
                status.error(
                    f"No se pudo descargar {label} tras {max_retries} intentos."
                )
                raise
        finally:
            info.empty()
            bar.empty()
            status.empty()


def safe_read_excel(bio_or_file, label: str = "archivo") -> pd.DataFrame:
    try:
        with st.spinner(f"Procesando {label}‚Ä¶"):
            df = pd.read_excel(bio_or_file, engine="openpyxl", dtype=str)
            if not isinstance(df, pd.DataFrame):
                raise ValueError("El archivo no es una hoja de c√°lculo v√°lida.")
            df = df.fillna("")
            return df
    except Exception as e:
        raise RuntimeError(f"No fue posible procesar {label}: {e}") from e


def get_index_or_first(options: List[str], value: str) -> int:
    try:
        return options.index(value)
    except Exception:  # pragma: no cover
        return 0


def _prepara_columnas(df: pd.DataFrame, cols: List[str]):
    for c in cols:
        if c in df.columns:
            df[c] = df[c].astype(str).fillna("")


def _prep_export(df: pd.DataFrame) -> pd.DataFrame:
    """
    Renombra columnas y elimina las administrativas antes de exportar.
    """
    out = df.copy()
    out = out.rename(
        columns={k: v for k, v in EXPORT_RENAME.items() if k in out.columns}
    )
    if "Url en LOCATE/IDEA" in out.columns and "Url de acceso" not in out.columns:
        out = out.rename(columns={"Url en LOCATE/IDEA": "Url de acceso"})
    drop_cols = [c for c in EXPORT_DROP_COLS if c in out.columns]
    if drop_cols:
        out = out.drop(columns=drop_cols)
    return out.fillna("")


def build_apa(row: pd.Series) -> str:
    """
    Generador APA simplificado usando los campos disponibles.
    """
    tit = str(row.get("T√≠tulo", "")).strip()

    # Autor(es) puede venir con o sin espacio final
    aut = ""
    for col in ["Autor(es)", "Autor(es) "]:
        if col in row.index:
            val = str(row.get(col, "")).strip()
            if val and val.upper() != "NO APLICA":
                aut = val
                break

    edit = str(row.get("Editorial", "")).strip()
    anio = str(row.get("A√±o de Publicaci√≥n", "")).strip()
    bd = str(row.get("Base de datos", "")).strip()
    url = str(row.get("Url OA", "") or row.get("Url de acceso", "")).strip()
    isbn = str(row.get("ISBN", "")).strip()
    issn = str(row.get("ISSN1", "")).strip()
    topog = str(row.get("No. Topogr√°fico", "")).strip()

    partes: list[str] = []
    if aut:
        partes.append(f"{aut}.")
    if anio and anio.upper() != "NO APLICA":
        partes.append(f"({anio}).")
    if tit:
        partes.append(f"{tit}.")
    if edit:
        partes.append(f"{edit}.")
    elif edit == "":
        partes.append("s.e.")

    acc: list[str] = []
    if bd:
        acc.append(f"Disponible en {bd}")
    if url:
        acc.append(url)
    if topog and topog.upper() != "NO APLICA":
        acc.append(f"No. Topogr√°fico: {topog}")
    if acc:
        partes.append("; ".join(acc) + ".")

    extras: list[str] = []
    if isbn and isbn.upper() != "NO APLICA":
        extras.append(f"ISBN: {isbn}")
    if issn and issn.upper() != "NO APLICA":
        extras.append(f"ISSN: {issn}")
    if extras:
        partes.append(" ".join(extras) + ".")

    return " ".join([p for p in partes if p]).replace("..", ".")


# CSS para cambiar el texto de "Browse files" (mejor esfuerzo)
st.markdown(
    """
<style>
button[title="Browse files"]{visibility: hidden;}
button[title="Browse files"]::after{
  content:" Cargar listado";
  visibility: visible;
  display:inline-block;
  padding:0.25rem 0.75rem;
  background:#2e7d32;
  color:white;
  border-radius:6px;
}
</style>
""",
    unsafe_allow_html=True,
)

# ---------------------------------- SIDEBAR ----------------------------------
with st.sidebar:
    st.image(LOGO_URL, use_container_width=True)
    st.caption(
        "Elaborado por David Camelo para la Biblioteca de la Universidad El Bosque"
    )

    st.markdown("### Plantillas oficiales:")
    st.markdown(f"- [Tem√°ticas]({URL_PLANTILLA_TEMATICAS})")
    st.markdown(f"- [T√©rminos a excluir]({URL_PLANTILLA_EXCLUSION})")

    # Archivos auxiliares s√≥lo para M√©todo A
    if ss.metodo == "A":
        st.markdown("### Archivos auxiliares (obligatorios)")
        bloqueados = ss.downloading or (not ss.bases_ready and ss.descarga_disparada)

        tem_up = st.file_uploader(
            "Tem√°ticas (.xlsx, col1=t√©rmino, col2=normalizado)",
            type=["xlsx"],
            key="tem_up_v82",
            disabled=bloqueados,
        )
        exc_up = st.file_uploader(
            "T√©rminos a excluir (.xlsx, col1)",
            type=["xlsx"],
            key="exc_up_v82",
            disabled=bloqueados,
        )

        if not bloqueados:
            if tem_up is not None:
                df = safe_read_excel(tem_up, "Tem√°ticas")
                ss.tematicas_df = df[[df.columns[0], df.columns[1]]].rename(
                    columns={
                        df.columns[0]: "termino",
                        df.columns[1]: "normalizado",
                    }
                ).fillna("")
                st.success(f"Tem√°ticas cargadas: {len(ss.tematicas_df)}")

            if exc_up is not None:
                df = safe_read_excel(exc_up, "T√©rminos a excluir")
                ss.excluir_df = df[[df.columns[0]]].rename(
                    columns={df.columns[0]: "excluir"}
                ).fillna("")
                st.success(f"T√©rminos a excluir cargados: {len(ss.excluir_df)}")
    else:
        st.markdown(
            "‚ÑπÔ∏è El **M√©todo B** no requiere cargar plantillas. "
            "Selecciona el **M√©todo A** si quieres usar listados de tem√°ticas."
        )

    st.markdown("---")
    # Subir bases manualmente s√≥lo si a√∫n no se han cargado/sincronizado
    if not ss.bases_ready:
        with st.expander(
            "‚ûï Avanzado: subir bases Digital/F√≠sica manualmente", expanded=False
        ):
            up_dig = st.file_uploader(
                "Base de datos de la colecci√≥n Digital (.xlsx)",
                type=["xlsx"],
                key="up_dig_v82",
            )
            up_fis = st.file_uploader(
                "Base de datos de la colecci√≥n F√≠sica (.xlsx)",
                type=["xlsx"],
                key="up_fis_v82",
            )

            if up_dig is not None:
                ss.df_digital = safe_read_excel(up_dig, "Colecci√≥n Digital")
                st.success("Colecci√≥n Digital (manual) cargada.")
            if up_fis is not None:
                ss.df_fisica = safe_read_excel(up_fis, "Colecci√≥n F√≠sica")
                st.success("Colecci√≥n F√≠sica (manual) cargada.")
            if ss.df_digital is not None and ss.df_fisica is not None:
                ss.bases_ready = True
                st.success("‚úÖ Bases oficiales listas en memoria (carga manual).")

# ---------------------------------- CUERPO PRINCIPAL ----------------------------------
st.title("Herramienta para la elaboraci√≥n de bibliograf√≠as especializadas")

# --- Bloque de informaci√≥n general ---
with st.expander("‚ÑπÔ∏è Informaci√≥n general", expanded=True):
    st.markdown(
        f"""
- **Objetivo:** permitir la autogesti√≥n por programa/asignatura/tema y resaltar **t√©rminos a excluir** para depuraci√≥n manual.  
- Usa siempre las bases oficiales (Digital/F√≠sica) o s√∫belas **manualmente** desde la barra lateral.  
- **Plantillas:** [Tem√°ticas]({URL_PLANTILLA_TEMATICAS}) y [T√©rminos a excluir]({URL_PLANTILLA_EXCLUSION}).  
- Los archivos adjuntos **no se almacenan** por la Universidad y se eliminan al cerrar la app.  
- El proceso puede tardar algunos minutos; **puedes seguir usando tu equipo** (no cierres el navegador).
        """
    )

# --- Sincronizaci√≥n de bases ---
st.markdown("#### Bases de datos de las colecciones de la Biblioteca")

if not ss.bases_ready:
    st.info(
        "Antes de buscar, sincroniza las bases de datos oficiales o carga los archivos "
        "desde la barra lateral (opci√≥n **Avanzado**)."
    )

    mid_col = st.columns([1, 2, 1])[1]
    with mid_col:
        btn_sync = st.button(
            "üîÑ Sincronizar bases de datos oficiales",
            type="primary",
            use_container_width=True,
            disabled=ss.downloading or ss.descarga_disparada,
        )

    if btn_sync and not ss.downloading:
        ss.descarga_disparada = True
        ss.downloading = True

    if ss.downloading:
        st.info(
            "Sincronizando colecciones **Digital** y **F√≠sica**‚Ä¶ "
            "Puedes seguir usando tu equipo. No cierres esta ventana."
        )

        # Digital
        st.subheader("Descargando Base de datos de la colecci√≥n Digital‚Ä¶")
        zona_dig = st.container()
        try:
            bio_d = download_with_resume(URL_DIGITAL, "Colecci√≥n Digital", zona_dig)
            st.caption("Colecci√≥n Digital: descarga completa. Verificando archivo‚Ä¶")
            ss.df_digital = safe_read_excel(bio_d, "Colecci√≥n Digital")
            st.success("Base de datos de la colecci√≥n Digital lista ‚úì")
        except Exception as e:
            st.error(f"No fue posible descargar la base Digital: {e}")
            ss.downloading = False

        # F√≠sica
        st.subheader("Descargando Base de datos de la colecci√≥n F√≠sica‚Ä¶")
        zona_fis = st.container()
        try:
            bio_f = download_with_resume(URL_FISICA, "Colecci√≥n F√≠sica", zona_fis)
            st.caption("Colecci√≥n F√≠sica: descarga completa. Verificando archivo‚Ä¶")
            ss.df_fisica = safe_read_excel(bio_f, "Colecci√≥n F√≠sica")
            st.success("Base de datos de la colecci√≥n F√≠sica lista ‚úì")
        except Exception as e:
            st.error(f"No fue posible descargar la base F√≠sica: {e}")
            ss.downloading = False

        if ss.df_digital is not None and ss.df_fisica is not None:
            ss.bases_ready = True
            ss.downloading = False
            st.success("‚úÖ Bases oficiales listas en memoria (sesi√≥n).")

if not ss.bases_ready:
    st.stop()
else:
    st.success("‚úÖ Bases oficiales listas en memoria (sesi√≥n).")

# ---------------------------------- SELECCI√ìN DE M√âTODO ----------------------------------
st.markdown("### Selecciona el modo de b√∫squeda")

metodo_label = st.radio(
    "",
    (
        "M√©todo A ‚Äì listado de tem√°ticas (plantilla)",
        "M√©todo B ‚Äì b√∫squeda avanzada tipo descubridor (experimental)",
    ),
    index=0 if ss.metodo == "A" else 1,
)
ss.metodo = "A" if metodo_label.startswith("M√©todo A") else "B"

# --- Paso a paso seg√∫n m√©todo ---
if ss.metodo == "A":
    with st.expander("üß≠ Paso a paso ‚Äì M√©todo A (listado de tem√°ticas)", expanded=True):
        st.markdown(
            f"""
**1) Sincronizaci√≥n (obligatoria una sola vez por sesi√≥n).**  
Si a√∫n no lo has hecho, usa **‚ÄúSincronizar bases de datos oficiales‚Äù** o carga las bases desde *Avanzado* en la barra lateral.

**2) Cargue sus tem√°ticas.**  
Descargue la plantilla de [Tem√°ticas]({URL_PLANTILLA_TEMATICAS}).  
La **columna 1** incluye variaciones del t√©rmino (con/sin tildes, otros idiomas).  
La **columna 2** agrupa/normaliza el t√©rmino, que ser√° el que ver√°s en los resultados.

**3) Cargue t√©rminos a excluir.**  
Use la plantilla de [T√©rminos a excluir]({URL_PLANTILLA_EXCLUSION}).  
Sirve para evitar falsos positivos (p. ej., buscar ‚Äúecolog√≠a‚Äù sin recuperar ‚Äúginecolog√≠a‚Äù).

**4) Par√°metros de b√∫squeda.**  
Por defecto la b√∫squeda se hace en **T√≠tulo** y **Tem√°ticas**, y se eliminan duplicados por **Url OA** (Digital) y **No. Topogr√°fico** (F√≠sica).  
Puedes cambiar estas columnas en la secci√≥n **Configuraci√≥n de b√∫squeda y duplicados**.

**5) Ejecute la b√∫squeda.**  
Pulse **‚ÄúüöÄ Iniciar b√∫squeda (M√©todo A)‚Äù**. Ver√° una tabla con los resultados (vista de hasta 200 filas por defecto).  
Podr√° **filtrar**, **marcar filas** y **exportar** en CSV/XLSX o **citas APA** (beta).

**6) Exportaciones y bit√°cora.**  
El Excel incluye la **bit√°cora por t√©rmino** y resalta coincidencias con **t√©rminos a excluir**.  
Las exportaciones ‚Äúsolo seleccionados‚Äù respetan lo marcado en la tabla.

**7) Nueva b√∫squeda.**  
Pulse **‚ÄúNueva b√∫squeda‚Äù** para cargar otras tem√°ticas y t√©rminos **sin re-sincronizar** las bases.
Al cerrar la pesta√±a, la sesi√≥n se pierde (no se guarda nada).
            """
        )
else:
    with st.expander(
        "üß≠ Paso a paso ‚Äì M√©todo B (b√∫squeda avanzada tipo descubridor)", expanded=True
    ):
        st.markdown(
            """
**1) Aseg√∫rese de tener las bases sincronizadas.**  
Use el bot√≥n **‚ÄúSincronizar bases de datos oficiales‚Äù** (o cargue los archivos manualmente desde la barra lateral).

**2) Defina el alcance.**  
Seleccione las colecciones a incluir (Digital/F√≠sica) y, si lo desea, limite por tipo de √≠tem normalizado  
(p. ej., s√≥lo **Libro**, s√≥lo **Revista**, etc.).

**3) Construya las condiciones.**  
Cada condici√≥n tiene:
- un **conector** (primera, Y / AND, O / OR, NO / NOT),  
- un **campo** (T√≠tulo, Autor(es), Tem√°ticas, Editorial, Cualquier campo),  
- un **operador** (contiene / no contiene) y  
- un **valor** de b√∫squeda.

Las condiciones se aplican en orden, sobre la misma tabla de resultados:
- **Y (AND)** filtra m√°s los resultados actuales.  
- **O (OR)** a√±ade nuevos resultados que cumplan la condici√≥n.  
- **NO (NOT)** excluye de los resultados actuales los registros que cumplan la condici√≥n.

**4) Ejecute la b√∫squeda avanzada.**  
Pulse **‚ÄúüöÄ Iniciar b√∫squeda avanzada (M√©todo B)‚Äù**. Ver√° una tabla con los t√≠tulos coincidentes.  
Podr√° **filtrar**, **marcar filas** y **exportar** en CSV/XLSX o **citas APA** (beta).

**5) Nueva b√∫squeda.**  
Use el bot√≥n **‚ÄúNueva b√∫squeda‚Äù** para limpiar condiciones y resultados, sin volver a sincronizar las bases.
            """
        )

# ---------------------------------- NUEVA B√öSQUEDA (limpia insumos/resultados, NO bases) ----------------------------------
col_nb = st.columns([1, 1, 4])[0]
with col_nb:
    if st.button("üß™ Nueva b√∫squeda", use_container_width=True):
        for k in (
            "tematicas_df",
            "excluir_df",
            "results_df",
            "bitacora_df",
            "b_conds",
        ):
            ss[k] = None if k != "b_conds" else []
        st.toast("Listo. Carga nuevos t√©rminos o ajusta las condiciones para buscar de nuevo.")


# ==========================================================================================
# M√âTODO A ‚Äì LISTADO DE TEM√ÅTICAS
# ==========================================================================================
def ejecutar_busqueda_metodo_a(col_busq1: str, col_busq2: str, col_dup_dig: str, col_dup_fis: str):  # noqa: E501
    if ss.tematicas_df is None or ss.excluir_df is None:
        st.warning(
            "Para usar el **M√©todo A** debes cargar **Tem√°ticas** y **T√©rminos a excluir** "
            "en la barra lateral."
        )
        return

    excluye = [
        str(x).strip()
        for x in ss.excluir_df["excluir"].tolist()
        if str(x).strip() != ""
    ]

    barra = st.progress(0)
    estado = st.empty()

    DF_D = ss.df_digital.copy()
    DF_F = ss.df_fisica.copy()

    # Asegurar columnas como texto
    _prepara_columnas(DF_D, [col_busq1, col_busq2, col_dup_dig])
    _prepara_columnas(DF_F, [col_busq1, col_busq2, col_dup_fis])

    def _buscar(
        df: pd.DataFrame,
        fuente: str,
        tem_df: pd.DataFrame,
        offset: int,
        total_steps: int,
    ) -> pd.DataFrame:
        res_list: list[pd.DataFrame] = []
        tem = tem_df.copy()
        tem["termino"] = tem["termino"].astype(str).fillna("")
        tem["normalizado"] = tem["normalizado"].astype(str).fillna("")
        N = len(tem)
        t0 = time.time()

        for i, row in tem.iterrows():
            term = normalize_text(row["termino"])
            if term:
                m1 = DF_D[col_busq1].map(
                    lambda s: term in normalize_text(s)
                ) if fuente == "Digital" else DF_F[col_busq1].map(
                    lambda s: term in normalize_text(s)
                )
                m2 = DF_D[col_busq2].map(
                    lambda s: term in normalize_text(s)
                ) if fuente == "Digital" else DF_F[col_busq2].map(
                    lambda s: term in normalize_text(s)
                )
                src = DF_D if fuente == "Digital" else DF_F
                md = src[m1 | m2].copy()
                if not md.empty:
                    md["Tem√°tica"] = row["termino"]
                    md["Tem√°tica normalizada"] = row["normalizado"]
                    md["Columna de coincidencia"] = None
                    md.loc[m1[m1].index, "Columna de coincidencia"] = col_busq1
                    md.loc[m2[m2].index, "Columna de coincidencia"] = md[
                        "Columna de coincidencia"
                    ].fillna(col_busq2)
                    md["Fuente"] = fuente
                    res_list.append(md)

            frac = (i + 1) / max(N, 1)
            elapsed = time.time() - t0
            est_total = elapsed / max(frac, 1e-6)
            est_rem = max(0, int(est_total - elapsed))
            barra.progress(min(1.0, (offset + i + 1) / total_steps))
            estado.info(
                f"{fuente}: {i+1}/{N} t√©rminos ‚Ä¢ transcurrido: {int(elapsed)} s ‚Ä¢ restante: {est_rem} s"  # noqa: E501
            )

        if res_list:
            return pd.concat(res_list, ignore_index=True)
        return pd.DataFrame()

    total = len(ss.tematicas_df) * 2
    res_d = _buscar(
        DF_D,
        "Digital",
        ss.tematicas_df,
        offset=0,
        total_steps=total,
    )
    res_f = _buscar(
        DF_F,
        "F√≠sica",
        ss.tematicas_df,
        offset=len(ss.tematicas_df),
        total_steps=total,
    )

    if not res_d.empty and col_dup_dig in res_d.columns:
        res_d = res_d.drop_duplicates(subset=[col_dup_dig], keep="first")
    if not res_f.empty and col_dup_fis in res_f.columns:
        res_f = res_f.drop_duplicates(subset=[col_dup_fis], keep="first")

    res = (
        pd.concat([res_d, res_f], ignore_index=True)
        if not (res_d.empty and res_f.empty)
        else pd.DataFrame()
    )

    ss.results_df = res

    # --- bit√°cora por t√©rmino ---
    tem = (
        ss.tematicas_df[["termino", "normalizado"]]
        .drop_duplicates()
        .reset_index(drop=True)
    )
    fuentes = pd.DataFrame({"Fuente": ["Digital", "F√≠sica"]})
    grid = fuentes.assign(key=1).merge(
        tem.assign(key=1), on="key"
    ).drop("key", axis=1)

    if res.empty:
        counts = pd.DataFrame(
            columns=["Fuente", "Tem√°tica", "Tem√°tica normalizada", "Resultados"]
        )
    else:
        counts = (
            res.groupby(["Fuente", "Tem√°tica", "Tem√°tica normalizada"], dropna=False)
            .size()
            .reset_index(name="Resultados")
        )

    bit = (
        grid.merge(
            counts,
            how="left",
            left_on=["Fuente", "termino", "normalizado"],
            right_on=["Fuente", "Tem√°tica", "Tem√°tica normalizada"],
        )
        .drop(columns=["Tem√°tica", "Tem√°tica normalizada"], errors="ignore")
        .rename(columns={"termino": "T√©rmino", "normalizado": "Normalizado"})
    )

    bit["Resultados"] = bit["Resultados"].fillna(0).astype(int)
    bit = bit.sort_values(
        ["Fuente", "Resultados", "T√©rmino"], ascending=[True, False, True]
    ).reset_index(drop=True)
    ss.bitacora_df = bit

    barra.progress(1.0)
    estado.empty()
    st.success("B√∫squeda finalizada (M√©todo A).")


# ==========================================================================================
# M√âTODO B ‚Äì B√öSQUEDA AVANZADA
# ==========================================================================================
CAMPOS_B = {
    "Cualquier campo": None,
    "T√≠tulo": "T√≠tulo",
    "Autor(es)": "Autor(es)",
    "Tem√°ticas": "Tem√°ticas",
    "Editorial": "Editorial",
    "A√±o de Publicaci√≥n": "A√±o de Publicaci√≥n",
}

OPERADORES_B = ["Contiene", "No contiene"]
CONECTORES_B = ["(primera)", "Y (AND)", "O (OR)", "NO (NOT)"]


def _mask_condicion(base: pd.DataFrame, campo: str | None, operador: str, valor: str):
    """
    Genera una m√°scara booleana para una condici√≥n simple sobre `base`.
    """
    val_norm = normalize_text(valor).lower()

    if campo is None:
        series_list = []
        for c in [
            "T√≠tulo",
            "Autor(es)",
            "Autor(es) ",
            "Tem√°ticas",
            "Editorial",
            "Base de datos",
        ]:
            if c in base.columns:
                s = base[c].fillna("").astype(str).map(
                    lambda x: val_norm in normalize_text(x).lower()
                )
                series_list.append(s)
        if not series_list:
            mask = pd.Series(False, index=base.index)
        else:
            mask = series_list[0]
            for s in series_list[1:]:
                mask = mask | s
    else:
        if campo not in base.columns:
            return pd.Series(False, index=base.index)
        series = base[campo].fillna("").astype(str)
        mask = series.map(lambda x: val_norm in normalize_text(x).lower())

    if operador == "Contiene":
        return mask
    elif operador == "No contiene":
        return ~mask
    else:
        return mask


def ejecutar_busqueda_metodo_b(
    colecciones: list[str],
    tipos_sel: list[str],
    condiciones: List[Dict[str, Any]],
):
    # Construir tabla base Digital + F√≠sica
    DF_D = ss.df_digital.copy()
    DF_F = ss.df_fisica.copy()
    DF_D["Fuente"] = "Digital"
    DF_F["Fuente"] = "F√≠sica"
    base = pd.concat([DF_D, DF_F], ignore_index=True)

    if colecciones:
        base = base[base["Fuente"].isin(colecciones)]

    if tipos_sel and TIPO_NORMAL_COL in base.columns:
        base = base[base[TIPO_NORMAL_COL].isin(tipos_sel)]

    # Nos aseguramos que todas las columnas relevantes sean texto
    _prepara_columnas(
        base,
        [
            "T√≠tulo",
            "Autor(es)",
            "Autor(es) ",
            "Tem√°ticas",
            "Editorial",
            "Base de datos",
            "A√±o de Publicaci√≥n",
        ],
    )

    res = None
    for idx, cond in enumerate(condiciones):
        valor = cond.get("valor", "").strip()
        if not valor:
            continue

        campo_key = cond.get("campo", "Cualquier campo")
        campo_col = CAMPOS_B.get(campo_key)
        operador = cond.get("operador", "Contiene")
        conector = cond.get("conector", "(primera)")

        mask = _mask_condicion(base, campo_col, operador, valor)

        # Primera condici√≥n: si el conector es NO se interpreta como "todo menos"
        if res is None:
            if conector.startswith("NO"):
                res = base[~mask].copy()
            else:
                res = base[mask].copy()
            continue

        # Resto de condiciones: se aplican sobre resultado actual y/o base
        if conector.startswith("Y"):
            # AND: se restringe el conjunto actual
            submask = mask.loc[res.index]
            res = res[submask].copy()
        elif conector.startswith("O"):
            # OR: se unen resultados actuales con nuevos hallazgos
            nuevos = base[mask].copy()
            res = (
                pd.concat([res, nuevos], ignore_index=True)
                .drop_duplicates()
                .reset_index(drop=True)
            )
        elif conector.startswith("NO"):
            # NOT: se excluyen del conjunto actual
            submask = mask.loc[res.index]
            res = res[~submask].copy()
        else:
            # Por si acaso, tratamos como nueva primera
            res = base[mask].copy()

    if res is None:
        res = base.copy()

    ss.results_df = res
    ss.bitacora_df = None
    st.success(f"B√∫squeda avanzada finalizada. Resultados: {len(res):,}")


# ==========================================================================================
# RENDERIZADO COM√öN DE RESULTADOS + EXPORTACIONES
# ==========================================================================================
def render_resultados(con_bitacora: bool):
    st.subheader("Resultados")

    if ss.results_df is None or ss.results_df.empty:
        st.info("A√∫n no hay resultados. Ejecuta una b√∫squeda.")
        return

    res = ss.results_df.copy()

    # Filtros r√°pidos
    colf1, colf2, colf3 = st.columns([1, 1, 2])
    with colf1:
        filtro_fuente = st.multiselect(
            "Fuente",
            options=sorted(res["Fuente"].dropna().unique().tolist())
            if "Fuente" in res.columns
            else [],
            default=None,
        )
    with colf2:
        col_tema_norm = "Tem√°tica normalizada"
        temas_norm = (
            sorted(res[col_tema_norm].dropna().unique().tolist())
            if col_tema_norm in res.columns
            else []
        )
        filtro_tema = st.multiselect(
            "Tem√°tica normalizada", options=temas_norm, default=None
        )
    with colf3:
        tipo_opts = (
            sorted(
                res.get(TIPO_NORMAL_COL, pd.Series(dtype=str))
                .dropna()
                .unique()
                .tolist()
            )
            if TIPO_NORMAL_COL in res.columns
            else []
        )
        filtro_tipo = st.multiselect(
            "Tipo normalizado", options=tipo_opts, default=None
        )

    if filtro_fuente:
        res = res[res["Fuente"].isin(filtro_fuente)]
    if filtro_tema and "Tem√°tica normalizada" in res.columns:
        res = res[res["Tem√°tica normalizada"].isin(filtro_tema)]
    if filtro_tipo and TIPO_NORMAL_COL in res.columns:
        res = res[res[TIPO_NORMAL_COL].isin(filtro_tipo)]

    st.caption(f"Filas totales (despu√©s de filtros): **{len(res):,}**")

    # Columna de selecci√≥n
    res_view = res.copy()
    if "__Seleccionar__" not in res_view.columns:
        res_view.insert(0, "__Seleccionar__", False)

    cva, cvb = st.columns([1, 1])
    with cva:
        show_all = st.checkbox("Mostrar todas las filas (Vista)", value=False)
    with cvb:
        limit = st.number_input(
            "Filas a mostrar (Vista)", min_value=50, max_value=10000, value=200, step=50
        )

    res_view_show = res_view if show_all else res_view.head(int(limit))

    res_view_show = st.data_editor(
        res_view_show,
        use_container_width=True,
        height=520,
        column_config={
            "__Seleccionar__": st.column_config.CheckboxColumn("Seleccionar"),
        },
        disabled=[c for c in res_view_show.columns if c != "__Seleccionar__"],
        key=f"data_editor_res_{ss.metodo}",
    )

    seleccion_mask = (
        res_view_show["__Seleccionar__"]
        if "__Seleccionar__" in res_view_show.columns
        else pd.Series(False, index=res_view_show.index)
    )
    seleccionados = res_view_show[seleccion_mask].drop(
        columns=["__Seleccionar__"], errors="ignore"
    )
    st.caption(f"Seleccionados en la vista: **{len(seleccionados):,}**")

    # ---------------------------------- Exportaciones ----------------------------------
    st.markdown("##### Exportaciones")
    colx1, colx2, colx3, colx4, colx5 = st.columns([1.2, 1.2, 1.6, 1.6, 2])

    # CSV completo (filtrado)
    with colx1:
        st.download_button(
            "‚¨áÔ∏è CSV (todo lo filtrado)",
            data=_prep_export(res).to_csv(index=False).encode("utf-8"),
            file_name="resultados_filtrados.csv",
            mime="text/csv",
            use_container_width=True,
        )

    # CSV de seleccionados
    with colx2:
        st.download_button(
            "‚¨áÔ∏è CSV (solo seleccionados)",
            data=_prep_export(
                seleccionados if not seleccionados.empty else res.head(0)
            )
            .to_csv(index=False)
            .encode("utf-8"),
            file_name="resultados_seleccionados.csv",
            mime="text/csv",
            disabled=seleccionados.empty,
            use_container_width=True,
        )

    # Excel completo
    with colx3:
        import xlsxwriter  # noqa: F401

        xbio = io.BytesIO()
        writer = pd.ExcelWriter(xbio, engine="xlsxwriter")

        res_x = _prep_export(res)
        res_x.to_excel(writer, index=False, sheet_name="Resultados")

        if con_bitacora:
            # Resaltado por t√©rminos a excluir (s√≥lo M√©todo A)
            if ss.excluir_df is not None:
                wb = writer.book
                ws = writer.sheets["Resultados"]
                fmt = wb.add_format({"bg_color": "#FFF599"})

                cols = list(res_x.columns)
                col_tit_name = (
                    EXPORT_RENAME.get(DEFAULT_COL_TITULO, DEFAULT_COL_TITULO)
                )
                col_tem_name = (
                    EXPORT_RENAME.get(DEFAULT_COL_TEMATICAS, DEFAULT_COL_TEMATICAS)
                )
                col_tit = cols.index(col_tit_name) + 1 if col_tit_name in cols else None
                col_tem = cols.index(col_tem_name) + 1 if col_tem_name in cols else None

                excluye = [
                    normalize_text(x)
                    for x in ss.excluir_df["excluir"].astype(str).tolist()
                    if str(x).strip()
                ]

                for r in range(1, len(res_x) + 1):
                    if col_tit is not None:
                        v = normalize_text(res_x.iloc[r - 1, col_tit - 1])
                        if any(t in v for t in excluye):
                            ws.write(r, col_tit - 1, res_x.iloc[r - 1, col_tit - 1], fmt)
                    if col_tem is not None:
                        v = normalize_text(res_x.iloc[r - 1, col_tem - 1])
                        if any(t in v for t in excluye):
                            ws.write(r, col_tem - 1, res_x.iloc[r - 1, col_tem - 1], fmt)

            if ss.bitacora_df is not None:
                ss.bitacora_df.to_excel(writer, index=False, sheet_name="Bit√°cora")

            writer.close()
            xbio.seek(0)
            st.download_button(
                "‚¨áÔ∏è Excel (filtrado + resaltado + Bit√°cora)",
                data=xbio.getvalue(),
                file_name="resultados_filtrados.xlsx",
                mime=(
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                ),
                use_container_width=True,
            )
        else:
            writer.close()
            xbio.seek(0)
            st.download_button(
                "‚¨áÔ∏è Excel (todo lo filtrado)",
                data=xbio.getvalue(),
                file_name="resultados_filtrados.xlsx",
                mime=(
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                ),
                use_container_width=True,
            )

    # Excel de seleccionados
    with colx4:
        if not seleccionados.empty:
            sel_x = _prep_export(seleccionados)
            bio_sel = io.BytesIO()
            with pd.ExcelWriter(bio_sel, engine="xlsxwriter") as wsel:
                sel_x.to_excel(wsel, index=False, sheet_name="Seleccionados")
            bio_sel.seek(0)
            st.download_button(
                "‚¨áÔ∏è Excel (solo seleccionados)",
                data=bio_sel.getvalue(),
                file_name="resultados_seleccionados.xlsx",
                mime=(
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                ),
                use_container_width=True,
            )
        else:
            st.download_button(
                "‚¨áÔ∏è Excel (solo seleccionados)",
                data=b"",
                file_name="resultados_seleccionados.xlsx",
                disabled=True,
                use_container_width=True,
            )

    # Citas APA para seleccionados
    with colx5:
        if not seleccionados.empty:
            citas = [build_apa(r) for _, r in seleccionados.iterrows()]
            txt = "\n\n".join(c for c in citas if c.strip())
            st.download_button(
                "üßæ Citas APA (seleccionados)",
                data=txt.encode("utf-8"),
                file_name="citas_apa.txt",
                mime="text/plain",
                use_container_width=True,
            )
        else:
            st.download_button(
                "üßæ Citas APA (seleccionados)",
                data=b"",
                file_name="citas_apa.txt",
                mime="text/plain",
                use_container_width=True,
                disabled=True,
            )


# ==========================================================================================
# L√ìGICA PRINCIPAL POR M√âTODO
# ==========================================================================================
if ss.metodo == "A":
    # --- Validaciones M√©todo A ---
    if ss.tematicas_df is None or ss.excluir_df is None:
        st.warning(
            "Para usar el **M√©todo A** debes cargar **Tem√°ticas** y **T√©rminos a excluir** "
            "en la barra lateral."
        )
    else:
        st.subheader("Configuraci√≥n de b√∫squeda y duplicados (M√©todo A)")

        cols_dig = list(ss.df_digital.columns)
        cols_fis = list(ss.df_fisica.columns)
        common_cols = sorted(set(cols_dig + cols_fis))

        c1, c2, c3, c4 = st.columns([1, 1, 1, 1])

        with c1:
            col_busq1 = st.selectbox(
                "B√∫squeda principal por",
                options=common_cols,
                index=get_index_or_first(common_cols, DEFAULT_COL_TITULO),
                key="col_busq1_v82",
            )

        with c2:
            col_busq2 = st.selectbox(
                "B√∫squeda complementaria por",
                options=common_cols,
                index=get_index_or_first(common_cols, DEFAULT_COL_TEMATICAS),
                key="col_busq2_v82",
            )

        with c3:
            col_dup_dig = st.selectbox(
                "Columna de duplicados en Colecci√≥n Digital",
                options=cols_dig,
                index=get_index_or_first(cols_dig, DEFAULT_DUP_DIGITAL),
                key="dup_dig_v82",
            )

        with c4:
            col_dup_fis = st.selectbox(
                "Columna de duplicados en Colecci√≥n F√≠sica",
                options=cols_fis,
                index=get_index_or_first(cols_fis, DEFAULT_DUP_FISICA),
                key="dup_fis_v82",
            )

        st.caption(
            "Por defecto se usan ‚ÄúT√≠tulo‚Äù y ‚ÄúTem√°ticas‚Äù, y duplicados por "
            "‚ÄúUrl OA‚Äù / ‚ÄúNo. Topogr√°fico‚Äù. Puedes cambiarlos si lo necesitas."
        )

        st.markdown("---")

        if st.button(
            "üöÄ Iniciar b√∫squeda (M√©todo A)",
            type="primary",
            use_container_width=True,
        ):
            try:
                ejecutar_busqueda_metodo_a(
                    col_busq1=col_busq1,
                    col_busq2=col_busq2,
                    col_dup_dig=col_dup_dig,
                    col_dup_fis=col_dup_fis,
                )
            except Exception as e:
                st.error(f"Ocurri√≥ un problema durante la b√∫squeda: {e}")

        # Mostrar resultados y bit√°cora
        render_resultados(con_bitacora=True)

        st.subheader("üìë Bit√°cora por t√©rmino")
        if ss.bitacora_df is None or ss.bitacora_df.empty:
            st.info("A√∫n no hay bit√°cora. Ejecuta la b√∫squeda del M√©todo A.")
        else:
            st.dataframe(ss.bitacora_df, use_container_width=True, height=360)
            st.download_button(
                "Descargar bit√°cora (.csv)",
                data=ss.bitacora_df.to_csv(index=False).encode("utf-8"),
                file_name="bitacora_por_termino.csv",
                mime="text/csv",
                use_container_width=True,
            )

else:
    # ======================= M√âTODO B ==========================
    st.subheader("B√∫squeda avanzada (M√©todo B ‚Äì experimental)")

    # Alcance de b√∫squeda
    colc1, colc2 = st.columns([1, 1])
    with colc1:
        colecciones = st.multiselect(
            "Colecciones a incluir",
            options=["Digital", "F√≠sica"],
            default=["Digital", "F√≠sica"],
        )
    with colc2:
        # tipos normalizados disponibles
        todos_tipos = sorted(
            pd.concat([ss.df_digital, ss.df_fisica], ignore_index=True)
            .get(TIPO_NORMAL_COL, pd.Series(dtype=str))
            .dropna()
            .unique()
            .tolist()
        )
        tipos_sel = st.multiselect(
            "Tipo de √≠tem normalizado",
            options=todos_tipos,
            default=todos_tipos,  # por defecto TODOS
        )

    st.markdown(
        "Define una o varias condiciones. Se aplican en orden y se combinan con **Y (AND)**, "
        "**O (OR)** o **NO (NOT)**."
    )

    # N√∫mero de condiciones
    ss.b_num_cond = int(
        st.number_input(
            "N√∫mero de condiciones",
            min_value=1,
            max_value=5,
            value=ss.b_num_cond or 1,
            step=1,
        )
    )

    # Asegurar lista de condiciones en estado
    conds: List[Dict[str, Any]] = ss.b_conds or []
    while len(conds) < ss.b_num_cond:
        conds.append(
            {
                "conector": "(primera)" if len(conds) == 0 else "Y (AND)",
                "campo": "T√≠tulo",
                "operador": "Contiene",
                "valor": "",
            }
        )
    if len(conds) > ss.b_num_cond:
        conds = conds[: ss.b_num_cond]

    # Render de cada condici√≥n
    for i in range(ss.b_num_cond):
        st.markdown(f"**Condici√≥n {i+1}**")
        c1, c2, c3, c4 = st.columns([1, 1, 1, 3])

        with c1:
            opciones_con = CONECTORES_B if i > 0 else ["(primera)"]
            conds[i]["conector"] = st.selectbox(
                "Conector",
                options=opciones_con,
                index=opciones_con.index(conds[i]["conector"])
                if conds[i]["conector"] in opciones_con
                else 0,
                key=f"b_con_{i}",
            )
        with c2:
            conds[i]["campo"] = st.selectbox(
                "Campo",
                options=list(CAMPOS_B.keys()),
                index=list(CAMPOS_B.keys()).index(conds[i]["campo"])
                if conds[i]["campo"] in CAMPOS_B
                else 0,
                key=f"b_cam_{i}",
            )
        with c3:
            conds[i]["operador"] = st.selectbox(
                "Operador",
                options=OPERADORES_B,
                index=OPERADORES_B.index(conds[i]["operador"])
                if conds[i]["operador"] in OPERADORES_B
                else 0,
                key=f"b_op_{i}",
            )
        with c4:
            conds[i]["valor"] = st.text_input(
                "Valor",
                value=conds[i]["valor"],
                key=f"b_val_{i}",
            )

        st.markdown("---")

    ss.b_conds = conds

    if st.button(
        "üöÄ Iniciar b√∫squeda avanzada (M√©todo B)",
        type="primary",
        use_container_width=True,
    ):
        try:
            ejecutar_busqueda_metodo_b(
                colecciones=colecciones,
                tipos_sel=tipos_sel,
                condiciones=conds,
            )
        except Exception as e:
            st.error(f"Ocurri√≥ un problema durante la b√∫squeda avanzada: {e}")

    # Resultados sin bit√°cora ni resaltado especial
    render_resultados(con_bitacora=False)

    st.subheader("üìë Bit√°cora por t√©rmino")
    st.info(
        "La bit√°cora detallada con resultados por t√©rmino s√≥lo se genera con el **M√©todo A** "
        "(listado de tem√°ticas)."
    )
